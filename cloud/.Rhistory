library(purrr)
library(tokenizers)
maxlen <- 40
# Data Preparation --------------------------------------------------------
# Retrieve text
path <- get_file(
'nietzsche.txt',
origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'
)
# Load, collapse, and tokenize text
text <- read_lines(path) %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
# Cut the text in semi-redundant sequences of maxlen characters
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)
# Vectorization
x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == dataset$sentece[[i]])
})
y[i,] <- as.integer(chars == dataset$next_char[[i]])
}
# Model Definition --------------------------------------------------------
model <- keras_model_sequential()
model %>%
layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
layer_dense(length(chars)) %>%
layer_activation("softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
head(x)
dim(x)
nrow(path)
path
nrow(text)
x
library(beepr)
library(keras)
librarY(tm)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
max_features <- 100
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 3
maxlen <- dim(x_train)[2]
#
# model <- keras_model_sequential() %>%
#   #layer_embedding(input_dim=max_features,
# 	#	 output_dim=embedding_dims, input_length = maxlen) %>%
#   #layer_dropout(rate=0.2) %>%
#   #layer_flatten(.) %>%
#   layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu"  )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
#
L1 = 0
L2 = 0
DO = 0
OP = "adam"
Node = 200
model <- keras_model_sequential() %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen ,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
layer_dropout(DO) %>%
# layer_dense(Node , activation = "relu", input_shape = maxlen,
#             kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
#             layer_dropout(DO) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = OP,
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
library(keras)
library(tm)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
max_features <- 100
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 3
maxlen <- dim(x_train)[2]
#
# model <- keras_model_sequential() %>%
#   #layer_embedding(input_dim=max_features,
# 	#	 output_dim=embedding_dims, input_length = maxlen) %>%
#   #layer_dropout(rate=0.2) %>%
#   #layer_flatten(.) %>%
#   layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu"  )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
#
L1 = 0
L2 = 0
DO = 0
OP = "adam"
Node = 200
model <- keras_model_sequential() %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen ,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
layer_dropout(DO) %>%
# layer_dense(Node , activation = "relu", input_shape = maxlen,
#             kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
#             layer_dropout(DO) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = OP,
metrics = "accuracy"
)
Node
epochs
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
Fit <- model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
Fit
names( Fit  )
Fit$metrics
save_model_hdf5(model, "blerg.h5" )
getwd()
save_model_hdf5
Fit
save_model_hdf5(Fit , "blerg.h5" )
save_model_hdf5(PP , "blerg.h5" )
save_model_hdf5(model , "blerg.h5" )
Fit
Str(Fit)
str(Fit)
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amdir <- "G:\\math\\514\\cloud\\reviews.csv"
amazon <- read.csv(  amdir , stringsAsFactors= F )
amazon$Text <- as.character(amazon$Text )
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
max_features <- 50
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results);gc()
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 15
maxlen <- dim(x_train)[2]
lr_schedule <- function(epoch, lr) {
if(epoch <= 2) {
0.4
} else if(epoch > 2 && epoch <= 4){
0.01
} else {
0.001
}}
lr_reducer <- callback_learning_rate_scheduler(lr_schedule)
#
# model <- keras_model_sequential() %>%
# 	layer_gru(300) %>%
# 		layer_dropout(0.2) %>%
# 	layer_gru(300 ) %>%
# 		layer_dropout(0.2) %>%
# 	layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
# 		layer_dropout(0.2) %>%
#  	layer_dense(400 , activation = "relu" )  %>%
# 		layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu" )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
gc()
NODES <- 30
model <- keras_model_sequential() %>%
layer_dense(400 , activation = "relu"  )  %>%
layer_dropout(0.2)  %>%
layer_lstm(units = 300) %>%
layer_dropout(0.2) %>%
layer_lstm(units = 300 ) %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
model <- keras_model_sequential() %>%
#layer_embedding(input_dim=max_features,
#	 output_dim=embedding_dims, input_length = maxlen) %>%
layer_dense(NODES , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>%
layer_dense(NODES , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(NODES , activation = "relu"  )  %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
callback_learning_rate_scheduler
lr_reducer <- callback_learning_rate_scheduler(lr_schedule , verbose= 1)
lr_reducer <- callback_learning_rate_scheduler(lr_schedule  )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 5 ,
callbacks = list(
lr_reducer  ),
validation_data = list(x_test, (ytest ) )
)
256*2
rm(list=ls());gc()
library(keras)
library(tm)
library(textclean)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
# blerg <- amazon[ which(amazon$Score == 5) ,][  sample(1:363122, 250000), ]$Id
# amazon <- ( amazon[ -which(amazon$Id %in% blerg ),  ] )
amazon$Summary <- as.character( amazon$Summary )
library(text2vec)
it_train = itoken(amazon$Text,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = amazon$Id,
progressbar = T)
)
vocab = create_vocabulary(it_train)
vocab = create_vocabulary(it_train)
vocab
samples <- amazon$Summary
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
it_train = itoken(samples,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = amazon$Id,
progressbar = T)
vocab = create_vocabulary(it_train)
vocab
prune_vocabulary(vocab, term_count_min = 2,
doc_proportion_max = 0.8)
library(stringr)
twitter <- read.csv("twitter.csv",header = F)
beep("coin")
head(twitter)
clean_tweet <- twitter$V6
clean_tweet = gsub("&amp", "", unclean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
rm(clean_tweet)
clean_tweet = gsub("&amp", "", twitter$V6)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet
twitter$V6 <- clean_tweet
head(twitter)
twitter <- twitter[,c(1,6)]
head(twitter)
head(twitter)
library(stringr)
twitter <- read.csv("twitter.csv",header = F)
beep("coin")
head(twitter)
library(beepr); beep("coin")
head(twitter)
twitter <- twitter[,c(1,6)]
head(twitter)
write.csv(twitter, "twitter.csv" , row.names = F)
# write.csv(twitter, "twitter.csv" , row.names = F)
clean_tweet = gsub("&amp", "", twitter[,ncol(twitter)])
head(clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("&amp", "", twitter[,ncol(twitter)])
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
head(clean_tweet)
clean_tweet[1]
gsub( "  ", " " clean_tweet[1] )
gsub( "  ", " ", clean_tweet[1] )
clean_tweet = gsub( "  ", " ", clean_tweet  )
head(clean_tweet)
write.csv(twitter, "twitter.csv" , row.names = F)
table(twitter[,1])
twitters <- twitter[ sample(1:nrow(twitter), 20000),]
head(twitters)
table(twitter[,1])
table(twitters[,1])
twitters[which(twitters[,1] == 4),]
twitters[which(twitters[,1] == 4),1] <- 1
table(twitters[,1])
twitters
write.csv(twitter, "tweet.csv" , row.names = F)
300/60
10*30
library(keras)
twitters
head(twitters)
max_features <- 50
gc()
twitters
max_features <- 50
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(twitters$V6)
tweets <- texts_to_matrix(tokenizer,twitters$V6, mode = "tfidf")
predict_savedmodel(tweets, 'binary.h5')
model <- load_model_hdf5("binary.h5")
model %>% predict(model )
model %>% predict(tweets )
head(twitters)
nrow(twitter)
nrow(twitters)
rownames(twitters) <- NULL
write.csv(twitters, "twitters.csv", row.names= F)
library(cloudml); setwd("G:\\math\\514\\cloud")
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
library(ggplot2)
ggplot(amazon, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) +
labs(title="Distribution of Ratings", x="Rating")
require(keras)
library(tm)
library(textclean)
library(SnowballC)
library(lattice)
custom_stopwords <- tm::stopwords("english")[-which(tm::stopwords("english")=="not" |
tm::stopwords("english")=="should" |
tm::stopwords("english")=="against"|
tm::stopwords("english")=="below"|
tm::stopwords("english")=="above"|tm::stopwords("english")=="again"|tm::stopwords("english")=="few"|tm::stopwords("english")=="most")]
summary.clean <- amazon$Summary %>%
tolower(.) %>%
removePunctuation(.,preserve_intra_word_contractions=TRUE, preserve_intra_word_dashes=TRUE) %>%
removeNumbers(.) %>%
stripWhitespace(.) %>%
replace_contraction(.) %>%
removeWords(., custom_stopwords) %>%
stemDocument(.)
head(summary.clean)
tokenizer.summary <- text_tokenizer(num_words = max_features) %>%
fit_text_tokenizer(summary.clean)
vocab <- tokenizer.summary$word_counts
barchart(sort(unlist(vocab), decreasing=TRUE)[20:1], col='lightblue', xlab="Term Frequency", main="Most frequently appearing words")
40 * 10
400/60
custom_stopwords <- tm::stopwords("english")[-which(tm::stopwords("english")=="not" |
tm::stopwords("english")=="should" |
tm::stopwords("english")=="against"|
tm::stopwords("english")=="below"|
tm::stopwords("english")=="above"|tm::stopwords("english")=="again"|tm::stopwords("english")=="few"|tm::stopwords("english")=="most")]
summary.clean <- amazon$Summary %>%
tolower(.) %>%
removePunctuation(.,preserve_intra_word_contractions=TRUE, preserve_intra_word_dashes=TRUE) %>%
removeNumbers(.) %>%
stripWhitespace(.) %>%
replace_contraction(.) %>%
removeWords(., custom_stopwords) %>%
stemDocument(.)
head(summary.clean)
tokenizer.summary <- text_tokenizer(num_words = max_features) %>%
fit_text_tokenizer(summary.clean)
vocab <- tokenizer.summary$word_counts
barchart(sort(unlist(vocab), decreasing=TRUE)[20:1], col='lightblue', xlab="Term Frequency", main="Most frequently appearing words")
max_features = 500
