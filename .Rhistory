x_train
rm(list=ls())
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amdir <- "C:\\Users\\Administrator\\Documents\\reviews.csv"
amazon <- read.csv(  amdir , stringsAsFactors= F )
amazon$Text <- as.character(amazon$Text )
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
max_features <- 500
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results);gc()
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 5
maxlen <- dim(x_train)[2]
model <- keras_model_sequential() %>%
#layer_embedding(input_dim=max_features,
#	 output_dim=embedding_dims, input_length = maxlen) %>%
#layer_dropout(rate=0.2) %>%
#layer_flatten(.) %>%
layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>% regularizer_l1_l2(l1 = 0.01, l2 = 0.01) %>%
layer_dense(400 , activation = "relu"  )  %>%
layer_dropout(0.2) %>% regularizer_l1_l2(l1 = 0.01, l2 = 0.01) %>%
layer_dense(5 , activation = "softmax" )
model <- keras_model_sequential() %>%
layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>%
layer_dense(400 , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
gc()
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 20 ,
validation_data = list(x_test, (ytest ) )
)
## RNN's and embedding? What is pad sequences doing
# model %>% save_model_weights_hdf5("adagrad.h5")
model <- keras_model_sequential() %>%
layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>%
layer_dense(400 , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(400 , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 20 ,
validation_data = list(x_test, (ytest ) )
)
model
model %>% save_model_weights_hdf5("asdagrad.h5")
rm(list=ls())
gc()
library(keras)
require("readr")
library(stringr)
library(purrr)
library(tokenizers)
maxlen <- 40
# Data Preparation --------------------------------------------------------
# Retrieve text
path <- get_file(
'nietzsche.txt',
origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'
)
# Load, collapse, and tokenize text
text <- read_lines(path) %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
# Cut the text in semi-redundant sequences of maxlen characters
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)
# Vectorization
x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == dataset$sentece[[i]])
})
y[i,] <- as.integer(chars == dataset$next_char[[i]])
}
# Model Definition --------------------------------------------------------
model <- keras_model_sequential()
model %>%
layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
layer_dense(length(chars)) %>%
layer_activation("softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
head(x)
dim(x)
nrow(path)
path
nrow(text)
x
library(beepr)
library(keras)
librarY(tm)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
max_features <- 100
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 3
maxlen <- dim(x_train)[2]
#
# model <- keras_model_sequential() %>%
#   #layer_embedding(input_dim=max_features,
# 	#	 output_dim=embedding_dims, input_length = maxlen) %>%
#   #layer_dropout(rate=0.2) %>%
#   #layer_flatten(.) %>%
#   layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu"  )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
#
L1 = 0
L2 = 0
DO = 0
OP = "adam"
Node = 200
model <- keras_model_sequential() %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen ,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
layer_dropout(DO) %>%
# layer_dense(Node , activation = "relu", input_shape = maxlen,
#             kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
#             layer_dropout(DO) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = OP,
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
library(keras)
library(tm)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
max_features <- 100
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 3
maxlen <- dim(x_train)[2]
#
# model <- keras_model_sequential() %>%
#   #layer_embedding(input_dim=max_features,
# 	#	 output_dim=embedding_dims, input_length = maxlen) %>%
#   #layer_dropout(rate=0.2) %>%
#   #layer_flatten(.) %>%
#   layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu"  )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
#
L1 = 0
L2 = 0
DO = 0
OP = "adam"
Node = 200
model <- keras_model_sequential() %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen ,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
layer_dropout(DO) %>%
# layer_dense(Node , activation = "relu", input_shape = maxlen,
#             kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
#             layer_dropout(DO) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = OP,
metrics = "accuracy"
)
Node
epochs
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
Fit <- model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
Fit
names( Fit  )
Fit$metrics
save_model_hdf5(model, "blerg.h5" )
getwd()
save_model_hdf5
Fit
save_model_hdf5(Fit , "blerg.h5" )
save_model_hdf5(PP , "blerg.h5" )
save_model_hdf5(model , "blerg.h5" )
Fit
Str(Fit)
str(Fit)
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amdir <- "G:\\math\\514\\cloud\\reviews.csv"
amazon <- read.csv(  amdir , stringsAsFactors= F )
amazon$Text <- as.character(amazon$Text )
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
max_features <- 50
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results);gc()
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 15
maxlen <- dim(x_train)[2]
lr_schedule <- function(epoch, lr) {
if(epoch <= 2) {
0.4
} else if(epoch > 2 && epoch <= 4){
0.01
} else {
0.001
}}
lr_reducer <- callback_learning_rate_scheduler(lr_schedule)
#
# model <- keras_model_sequential() %>%
# 	layer_gru(300) %>%
# 		layer_dropout(0.2) %>%
# 	layer_gru(300 ) %>%
# 		layer_dropout(0.2) %>%
# 	layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
# 		layer_dropout(0.2) %>%
#  	layer_dense(400 , activation = "relu" )  %>%
# 		layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu" )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
gc()
NODES <- 30
model <- keras_model_sequential() %>%
layer_dense(400 , activation = "relu"  )  %>%
layer_dropout(0.2)  %>%
layer_lstm(units = 300) %>%
layer_dropout(0.2) %>%
layer_lstm(units = 300 ) %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
model <- keras_model_sequential() %>%
#layer_embedding(input_dim=max_features,
#	 output_dim=embedding_dims, input_length = maxlen) %>%
layer_dense(NODES , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>%
layer_dense(NODES , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(NODES , activation = "relu"  )  %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
callback_learning_rate_scheduler
lr_reducer <- callback_learning_rate_scheduler(lr_schedule , verbose= 1)
lr_reducer <- callback_learning_rate_scheduler(lr_schedule  )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 5 ,
callbacks = list(
lr_reducer  ),
validation_data = list(x_test, (ytest ) )
)
256*2
amdir <- "G:\\math\\514\\cloud\\reviews.csv"
amazon <- read.csv(  amdir , stringsAsFactors= F )
amazon$Text <- as.character(amazon$Text )
nrow(amazon)
length(unique(amazon$ProductId))
length(unique(amazon$UserId))
length(unique(amazon$ProfileName))
#Example of rating 1 review
amazon[which(amazon$Score==1)[1],c(7,9,10)]
#Example of rating 2 review
amazon[which(amazon$Score==2)[1],c(7,9,10)]
#Example of rating 3 review
amazon[which(amazon$Score==3)[1],c(7,9,10)]
#Example of rating 4 review
amazon[which(amazon$Score==4)[1],c(7,9,10)]
#Example of rating 5 review
amazon[which(amazon$Score==5)[1],c(7,9,10)]
#Distribution of ratings
library(ggplot2)
ggplot(amazon, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) + labs(title="Distribution of Ratings", x="Rating")
#Max. number of words in a single full text / summary
library(ngram)
#summary text
wc_distrib.summary <- lapply(as.character(amazon$Summary), function(x) wordcount(x))
summary(unlist(wc_distrib.summary))
#full text
wc_distrib.full <- lapply(as.character(amazon$Text), function(x) wordcount(x))
summary(unlist(wc_distrib.full))
wc_distrib.df <- data.frame(wordcount=c(unlist(wc_distrib.summary),unlist(wc_distrib.full)),
text_type=c(rep('Summary',length(unlist(wc_distrib.summary))), rep('Full',length(unlist(wc_distrib.summary)))))
ggplot(wc_distrib.df,  aes(x=wordcount, col=text_type)) + geom_histogram(fill="white", position="dodge",binwidth=5) + xlim(0,200) + ylim(0,60000) + labs(title="Distribution of Word Count", x="Word Count/Review", y="Number of Reviews")
wc_distrib.df
getwd()
setwd("G:\\math\\514")
write.csv(wc_distrib.df,"wc_distrib.csv")
write.csv(wc_distrib.df,"wc_distrib.csv",rownames = F)
write.csv(wc_distrib.df,"wc_distrib.csv",row.names = F)
options(htmltools.dir.version = FALSE)
# find.image searches subdirectories for partial match of argument. Useful when some images are dynamically generated
find.image=function(name){grep(name,list.files(recursive = TRUE),value=TRUE)}
# Example of how to use find.image to display an image
#![:scaleimgpct 100%](`r find.image("mnist_digits")`)
text_tbl <- data.frame( c("Summary","Full"), c(4,56),c(42,3432))
kable(text_tbl,align=c( rep('c',times=3) ) ,
escape = FALSE,
col.names = c("Text Type", "Median Word Count / Review", "Max Word Count / Review") )  ) %>%
kable(text_tbl,align=c( rep('c',times=3) ) ,
escape = FALSE,
col.names = c("Text Type", "Median Word Count / Review", "Max Word Count / Review") )
library(kableExtra)
text_tbl <- data.frame( c("Summary","Full"), c(4,56),c(42,3432))
kable(text_tbl,align=c( rep('c',times=3) ) ,
escape = FALSE,
col.names = c("Text Type", "Median Word Count / Review", "Max Word Count / Review") )    %>%
kable_styling(full_width = F, font_size = 18) %>%
column_spec(1,width = "10 em", bold = T ) %>%
column_spec(2, width = "10em", bold = F   )  %>%
column_spec(3, width = "10em" , bold = F   )   %>%
row_spec(c(2,4),  background = "#e7ebea")%>%
row_spec(c(1,3,5),  background = "#cbd7d5") %>%
row_spec(0, align = "c")
wc_distrib.df <- read.csv("wc_distrib.csv")
ggplot(wc_distrib.df,  aes(x=wordcount, col=text_type)) +
geom_histogram(fill="white", position="dodge",binwidth=5) + xlim(0,200) +
ylim(0,60000) + labs(title="Distribution of Word Count", x="Word Count/Review", y="Number of Reviews")
hamazon <- read.csv("G:\\math\\514\\template\\hamazon.csv",header = T)
hamazon[,ncol(hamazon)] <- as.character(hamazon[ ,ncol(hamazon)])
text_tbl <- hamazon[1,]
text_tbl
text_tbl[,1:9]
text_tbl[,1:8]
text_tbl
amazon[1,]
text_tbl <- data.frame( c("Product Id","Profile Name", "Helpfulness of Review","Time of Review", "Product Rating",
"Summary","Full Review"),
c("B001E4KFG0","delmartian","1","1303862400","5","Good Quality Dog Food",
"I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.
"))
text_tbl
kable(text_tbl,align=c( rep('c',times=3) ) ,
escape = FALSE,
col.names = c("Text Type", "Median Word Count / Review" ) )    %>%
kable_styling(full_width = F, font_size = 18) %>%
column_spec(1,width = "5em", bold = T ) %>%
column_spec(2, width = "8em", bold = T   )  %>%
